# Experiment params
env_id: mo-walker2d-v4
max_episode_steps: 100
one_hot_wrapper: False
gamma: 0.99
study_name: sn_ddpg_dst
seed: 1
wandb_project_name: IPRO_walker_neurips
wandb_entity: null
n_trials: 1


# Outer loop params
outer_loop:
  method: IPRO-2D
  tolerance: 1.e-15
  max_iterations: null
  track: True


# Oracle params
oracle:
  algorithm: SN-MO-DDPG
  aug: 0.01
  scale: 100
  grid_sample: True
  actor_activation: relu
  critic_activation: relu
  pretrain_iters: 25
  online_train_freq: 10
  pre_train_freq: 10
  target_update_freq: 1
  gradient_steps: 1
  pre_learning_start: 100
  pre_epsilon_start: 1.0
  pre_epsilon_end: 0.05
  pre_exploration_frac: 0.5
  online_learning_start: 100
  online_epsilon_start: 1.0
  online_epsilon_end: 0.05
  online_exploration_frac: 0.3
  buffer_size: 500000
  batch_size: 128
  tau: 0.1
  num_referents: 4
  eval_episodes: 100
  log_freq: 500
  track: False


# Hyperparameter options to be selected during the optimisation
hyperparameters:
  lr_actor:
    type: categorical
    choices: [ 0.0007, 0.0005, 0.0004, 0.0003, 0.0002, 0.0001 ]
  lr_critic:
    type: categorical
    choices: [ 0.0007, 0.0005, 0.0004, 0.0003, 0.0002, 0.0001 ]
  hidden_size_actor:
    type: categorical
    choices: [ 128, 256 ]
  hidden_size_critic:
    type: categorical
    choices: [ 128, 256 ]
  num_hidden_layers_actor:
    type: categorical
    choices: [ 2, 3, 4 ]
  num_hidden_layers_critic:
    type: categorical
    choices: [ 2, 3, 4 ]
  pretraining_steps:
    type: categorical
    choices: [ 10000, 20000 ]
  online_steps:
    type: categorical
    choices: [ 5000, 10000 ]
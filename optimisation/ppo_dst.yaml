# Experiment params
env_id: deep-sea-treasure-concave-v0
max_episode_steps: 50
one_hot_wrapper: True
gamma: 1.0
study_name: ppo_dst
seed: [ 0, 1, 2, 3, 4 ]
wandb_project_name: IPRO_opt
wandb_entity: null
n_trials: 10


# Outer loop params
outer_loop:
  method: IPRO-2D
  tolerance: 0.0
  max_iterations: null
  track: True


# Oracle params
oracle:
  algorithm: MO-PPO
  warm_start: False
  eval_episodes: 1
  log_freq: 2000
  track: False


# Hyperparameter options to be selected during the optimisation
hyperparameters:
  aug:
    type: categorical
    choices: [ 0, 0.001, 0.005, 0.01, 0.1 ]
  scale:
    type: categorical
    choices: [ 1, 10, 100, 500, 1000 ]
  global_steps:
    type: categorical
    choices: [ 3.e+5, 4.e+5, 5.e+5 ]
  num_hidden_layers:
    type: int
    min: 1
    max: 3
  hidden_size:
    type: categorical
    choices: [ 32, 64, 128, 256 ]
  lr_actor:
    type: categorical
    choices: [ 1.e-4, 3.e-4, 5.e-4, 7.e-4, 1.e-3 ]
  lr_critic:
    type: categorical
    choices: [ 1.e-4, 3.e-4, 5.e-4, 7.e-4, 1.e-3 ]
  e_coef:
    type: categorical
    choices: [ 0.001, 0.005, 0.01, 0.05, 0.1 ]
  v_coef:
    type: categorical
    choices: [ 0.1, 0.3, 0.5 ]
  max_grad_norm:
    type: categorical
    choices: [ 0.5, 1.0, 5.0, 10.0, 50.0 ]
  normalize_advantage:
    type: categorical
    choices: [ True, False ]
  n_steps:
    type: categorical
    choices: [ 16, 32, 64, 128, 256 ]
  gae_lambda:
    type: categorical
    choices: [ 0.5, 0.95, 1. ]
  num_envs:
    type: categorical
    choices: [ 2, 4, 8 ]
  num_minibatches:
    type: categorical
    choices: [ 1, 2, 4, 8 ]
  update_epochs:
    type: categorical
    choices: [ 1, 2, 4, 8 ]
  clip_coef:
    type: categorical
    choices: [ 0.1, 0.2, 0.3, 0.4, 0.5 ]
  clip_range_vf:
    type: categorical
    choices: [ 0.1, 0.2, 0.3, 0.4, 0.5 ]
  anneal_lr:
    type: categorical
    choices: [ True, False ]

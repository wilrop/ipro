# Experiment params
env_id: mo-reacher-v4
max_episode_steps: 50
one_hot_wrapper: False
gamma: 0.99
study_name: sn_dqn_reacher
seed: 1
wandb_project_name: IPRO_opt
wandb_entity: null
n_trials: 1


# Outer loop params
outer_loop:
  method: IPRO
  tolerance: 1.e-15
  max_iterations: 2000
  track: True


# Oracle params
oracle:
  algorithm: SN-MO-DQN
  aug: 0.01
  scale: 100
  online_train_freq: 1
  pre_train_freq: 4
  target_update_freq: 1
  gradient_steps: 1
  pre_learning_start: 1000
  online_learning_start: 100
  online_epsilon_start: 0.5
  online_epsilon_end: 0.05
  online_exploration_frac: 0.25
  tau: 0.1
  buffer_size: 100000
  eval_episodes: 100
  log_freq: 500
  track: False


# Hyperparameter options to be selected during the optimisation
hyperparameters:
  lr:
    type: categorical
    choices: [ 0.001 , 0.0007, 0.0003,  0.0001 ]
  num_hidden_layers:
    type: int
    min: 1
    max: 3
  hidden_size:
    type: categorical
    choices: [ 64, 128, 256 ]
  pre_epsilon_start:
    type: categorical
    choices: [ 0.2, 0.5, 0.75, 1.0 ]
  pre_epsilon_end:
    type: categorical
    choices: [ 0.05, 0.1, 0.2 ]
  pre_exploration_frac:
    type: categorical
    choices: [ 0.25, 0.5, 0.75 ]
  pretraining_steps:
    type: categorical
    choices: [ 7500, 10000, 15000, 20000, 25000 ]
  online_steps:
    type: categorical
    choices: [ 2500, 5000, 7500 ]
  pretrain_iters:
    type: categorical
    choices: [ 25, 50, 75, 100 ]
  num_referents:
    type: categorical
    choices: [ 4, 8, 16, 32 ]
  batch_size:
    type: categorical
    choices: [ 16, 32, 64 ]
env_id: mo-reacher-v4
nadirs: [ [ -50, -50, -50, -50 ], [ -50, -50, -50, -50 ], [ -50, -50, -50, -50 ], [ -50, -50, -50, -50 ] ]
ideals: [ [ 40, -50, -50, -50 ], [ -50, 40, -50, -50 ], [ -50, -50, 40, -50 ], [ -50, -50, -50, 40 ] ]
ref_point: [ -50, -50, -50, -50 ]
outer_loop: PRIOL
tolerance: 0.0001
oracle: MO-DQN
gamma: 0.99
study_name: dqn_reacher
seed: 1
wandb_project_name: IPRO
wandb_entity: null
track: True
warm_start: False
n_trials: 100
max_episode_steps: 50
log_freq: 1000

hyperparameters:
  aug:
    type: categorical
    choices: [ 0, 0.001, 0.005, 0.01, 0.1 ]
  scale:
    type: categorical
    choices: [ 1, 10, 100, 500, 1000 ]
  global_steps:
    type: categorical
    choices: [ 5.e+4, 1.e+5, 5.e+5, 1.e+6 ]
  eval_episodes:
    type: constant
    value: 100
  num_hidden_layers:
    type: int
    min: 1
    max: 3
  hidden_size:
    type: categorical
    choices: [ 32, 64, 128, 256 ]
  lr:
    type: categorical
    choices: [ 1.e-4, 3.e-4, 5.e-4, 7.e-4, 1.e-3 ]
  buffer_size:
    type: categorical
    choices: [ 10000, 50000, 100000 ]
  epsilon_start:
    type: categorical
    choices: [ 0.5, 0.7, 1.0 ]
  learning_start:
    type: categorical
    choices: [ 1000, 2000, 5000, 10000 ]
  epsilon_end:
    type: categorical
    choices: [ 0.05, 0.1, 0.2 ]
  exploration_frac:
    type: categorical
    choices: [ 0.1, 0.3, 0.5, 0.7 ]
  batch_size:
    type: categorical
    choices: [ 32, 64 ]
  per:
    type: categorical
    choices: [ True, False ]
  conditionals:
    - vars: [ per ]
      cond: per
      if_cond:
        alpha_per:
          type: categorical
          choices: [ 0.6, 0.8, 1. ]
        min_priority:
          type: constant
          value: 0.001
      else:
        alpha_per:
          type: constant
          value: 0.6
        min_priority:
          type: constant
          value: 0.001
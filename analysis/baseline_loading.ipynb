{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "api = wandb.Api(timeout=120)\n",
    "runs = api.runs(\"wilrop/MORL-Baselines\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d77755730d504ac3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_hists = {\n",
    "    'deep-sea-treasure-concave-v0': {'PCN': [], 'GPI-LS': [], 'Envelope': []},\n",
    "    'minecart-v0': {'PCN': [], 'GPI-LS': [], 'Envelope': []},\n",
    "    'mo-reacher-v4': {'PCN': [], 'GPI-LS': [], 'Envelope': []}\n",
    "}\n",
    "\n",
    "for run in runs:\n",
    "    env_id = run.config['env_id']\n",
    "    alg = run.config['algo']\n",
    "    run_hists[env_id][alg].append(run.history(keys=['eval/hypervolume', 'global_step']))\n",
    "    print(f'Added run to {env_id} - {alg}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e10fcabacabb10b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fill_hvs(hists, num_samples):    \n",
    "    min_logs = min(len(hist['eval/hypervolume']) for hist in hists)\n",
    "    indices = np.linspace(0, min_logs - 1, num_samples).astype(int)\n",
    "    all_global_steps = []\n",
    "    \n",
    "    for hist in hists:\n",
    "        steps = hist['global_step'].to_numpy()\n",
    "        new_steps = steps[indices]\n",
    "        all_global_steps.append(new_steps)\n",
    "        \n",
    "    global_steps = np.sort(np.unique(np.concatenate(all_global_steps)))\n",
    "    all_hypervolumes = []\n",
    "    \n",
    "    for hist in hists:\n",
    "        new_hv = []\n",
    "        hypervolumes = hist['eval/hypervolume'].to_numpy()\n",
    "        hv_steps = hist['global_step'].to_numpy()\n",
    "        for step in global_steps:\n",
    "            idx = np.argmin(np.abs(hv_steps - step))\n",
    "            new_hv.append(hypervolumes[idx])\n",
    "        new_hv = np.insert(new_hv, 0, 0)\n",
    "        all_hypervolumes.append(new_hv.tolist())\n",
    "    \n",
    "    global_steps = np.insert(global_steps, 0, 0)\n",
    "    return global_steps, all_hypervolumes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec85e956ed627e83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_samples = 300\n",
    "num_seeds = 5\n",
    "for env_id, alg_hists in run_hists.items():\n",
    "    for alg, hists in alg_hists.items():\n",
    "        if not hists:\n",
    "            continue\n",
    "        global_steps, hvs = fill_hvs(hists, num_samples)\n",
    "        hv_dict = {alg: [], 'Step': [], 'Seed': []}\n",
    "            \n",
    "        for seed, hv in enumerate(hvs):\n",
    "            hv_dict[alg].extend(hv)\n",
    "            hv_dict['Step'].extend(global_steps)\n",
    "            hv_dict['Seed'].extend([seed] * len(global_steps))\n",
    "\n",
    "        hv_df = pd.DataFrame.from_dict(hv_dict)\n",
    "        print(f\"Saving data for {alg}\")\n",
    "        hv_df.to_csv(f'data/{alg}_{env_id}_hv.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4729d1dd205473b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "739ac0ea19edadc5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

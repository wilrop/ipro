{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup experiments to extract data from.\n",
    "api = wandb.Api(timeout=120)\n",
    "runs = api.runs(\"wilrop/IPRO_runs\") \n",
    "ppo_grid_runs = api.runs(\"wilrop/IPRO_ppo_grid\")\n",
    "a2c_grid_runs = api.runs(\"wilrop/IPRO_a2c_grid\")\n",
    "algs = [\"SN-MO-PPO\", \"SN-MO-DQN\", \"SN-MO-A2C\"]\n",
    "env_ids = [\"deep-sea-treasure-concave-v0\", \"minecart-v0\", \"mo-reacher-v4\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Group runs based on Parent runs, env_id and algorithm.\n",
    "run_hists = {env_id: {alg: defaultdict(list) for alg in algs} for env_id in env_ids}\n",
    "\n",
    "for run in chain(runs, ppo_grid_runs, a2c_grid_runs):\n",
    "    env_id = run.config['env_id']\n",
    "    alg = run.config['alg_name']\n",
    "    if env_id in env_ids and alg in algs:\n",
    "        if 'parent_run_id' in run.config:\n",
    "            group = run.config['parent_run_id']\n",
    "        else:\n",
    "            group = run.config['group']\n",
    "        run_hists[env_id][alg][group].append(run)\n",
    "        print(f'Added run to {env_id} - {alg}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Keep only the best runs.\n",
    "best_data = {env_id: {alg: None for alg in algs} for env_id in env_ids}\n",
    "max_iterations = {env_id: 0 for env_id in env_ids}\n",
    "leftovers = []\n",
    "\n",
    "for env_id in run_hists:\n",
    "    for alg in run_hists[env_id]:\n",
    "        best_hv = -1\n",
    "        for group, runs in run_hists[env_id][alg].items():\n",
    "            hvs = [run.summary['outer/hypervolume'] for run in runs]\n",
    "            mean_hv = np.mean(hvs)\n",
    "            if mean_hv > best_hv:\n",
    "                if len(runs) == 5:\n",
    "                    best_hv = mean_hv\n",
    "                    best_data[env_id][alg] = runs\n",
    "                elif len(runs) < 5:\n",
    "                    print(f\"Adding {alg} - {env_id} with mean {mean_hv} and {len(runs)} runs to leftovers\")\n",
    "                    processed_seeds = [run.config['seed'] for run in runs]\n",
    "                    for seed in range(5):\n",
    "                        if seed not in processed_seeds:\n",
    "                            leftovers.append([alg, env_id, seed, group])\n",
    "                else:\n",
    "                    print(f\"Skipping {alg} - {env_id} - {group} with mean {mean_hv} and {len(runs)} runs\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save the runs to try and best runs to JSON files.      \n",
    "leftovers = {idx: tpl for idx, tpl in enumerate(leftovers, 1)}\n",
    "print(f\"Number of runs leftover: {len(leftovers)}\")\n",
    "json.dump(leftovers, open('../experiments/evaluation/leftovers.json', 'w'))\n",
    "json.dump({env_id: {alg: ['/'.join(run.path) for run in runs] for alg, runs in best_data[env_id].items()} for env_id in best_data}, open('data/best_runs.json', 'w'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Print best results\n",
    "for env_id in best_data:\n",
    "    for alg, runs in best_data[env_id].items():\n",
    "        hvs = np.array([run.summary['outer/hypervolume'] for run in runs])\n",
    "        print(f\"Best run for {alg} - {env_id} | Mean HV: {np.mean(hvs)} - HVs: {hvs}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extract the maximum number of iterations.\n",
    "max_iterations = {env_id: {alg: 0 for alg in algs} for env_id in env_ids}\n",
    "for env_id in best_data:\n",
    "    for alg, runs in best_data[env_id].items():\n",
    "        for run in runs:\n",
    "            df = run.history(keys=['iteration'])\n",
    "            if not df.empty:\n",
    "                iters = df.iloc[-1]['iteration'] + 1\n",
    "                max_iterations[env_id][alg] = max(max_iterations[env_id][alg], iters)\n",
    "print(max_iterations)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def fill_iterations(hypervolumes, coverages, max_iter):\n",
    "    \"\"\"Fill the hypervolume and coverage lists with the last value to have the same length as max_iter.\n",
    "    \n",
    "    Args:\n",
    "        hypervolumes (list): List of hypervolumes.\n",
    "        coverages (list): List of coverages.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    while len(hypervolumes) < max_iter:\n",
    "        hypervolumes.append(hypervolumes[-1])\n",
    "        coverages.append(coverages[-1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Make dictionaries with the data and save to csv.\n",
    "for env_id in best_data:\n",
    "    for alg, runs in best_data[env_id].items():\n",
    "        max_iter = max_iterations[env_id][alg]\n",
    "        hv_dict = {alg: [], 'Step': [], 'Seed': []}\n",
    "        cov_dict = {alg: [], 'Step': [], 'Seed': []}\n",
    "\n",
    "        for seed, run in enumerate(runs):\n",
    "            hist = run.history(keys=['outer/hypervolume', 'outer/coverage'])\n",
    "            hypervolumes = hist['outer/hypervolume'].values.tolist()\n",
    "            coverages = list(np.clip(hist['outer/coverage'].values.tolist(), 0, 1))\n",
    "            step_size = run.config['online_steps']\n",
    "            fill_iterations(hypervolumes, coverages, max_iter)\n",
    "            last_iter = max_iter\n",
    "            global_steps = np.arange(last_iter) * step_size\n",
    "            global_steps = global_steps.tolist()\n",
    "            hv_dict[alg].extend(hypervolumes)\n",
    "            cov_dict[alg].extend(coverages)\n",
    "            hv_dict['Step'].extend(global_steps)\n",
    "            cov_dict['Step'].extend(global_steps)\n",
    "            hv_dict['Seed'].extend([seed] * last_iter)\n",
    "            cov_dict['Seed'].extend([seed] * last_iter)\n",
    "\n",
    "        hv_df = pd.DataFrame.from_dict(hv_dict)\n",
    "        cov_df = pd.DataFrame.from_dict(cov_dict)\n",
    "        print(f\"Saving data for {env_id} - {alg}\")\n",
    "        hv_df.to_csv(f'data/{alg}_{env_id}_hv.csv', index=False)\n",
    "        cov_df.to_csv(f'data/{alg}_{env_id}_cov.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

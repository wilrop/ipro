diff --git a/experiments/a2c_dst.py b/experiments/a2c_dst.py
index 1957cab..c80c11c 100644
--- a/experiments/a2c_dst.py
+++ b/experiments/a2c_dst.py
@@ -3,6 +3,7 @@ import random
 import torch
 import argparse
 import time
+import wandb
 
 import numpy as np
 
@@ -11,6 +12,7 @@ from experiments import setup_env
 from linear_solvers import init_linear_solver
 from oracles import init_oracle
 from outer_loops import init_outer_loop
+from torch.utils.tensorboard import SummaryWriter
 
 
 def parse_args():
@@ -24,7 +26,7 @@ def parse_args():
                         help="if toggled, `torch.backends.cudnn.deterministic=False`")
     parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
                         help="if toggled, cuda will be enabled by default")
-    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
                         help="if toggled, this experiment will be tracked with Weights and Biases")
     parser.add_argument("--wandb-project-name", type=str, default="PRIOL", help="the wandb's project name")
     parser.add_argument("--wandb-entity", type=str, default=None, help="the entity (team) of wandb's project")
@@ -36,9 +38,9 @@ def parse_args():
     parser.add_argument("--env_id", type=str, default="deep-sea-treasure-concave-v0", help="The game to use.")
     parser.add_argument('--outer_loop', type=str, default='2D', help='The outer loop to use.')
     parser.add_argument("--oracle", type=str, default="MO-A2C", help="The algorithm to use.")
-    parser.add_argument("--aug", type=float, default=0.001, help="The augmentation term in the utility function.")
+    parser.add_argument("--aug", type=float, default=0.005, help="The augmentation term in the utility function.")
     parser.add_argument("--tolerance", type=float, default="1e-4", help="The tolerance for the outer loop.")
-    parser.add_argument("--warm_start", type=bool, default=False, help="Whether to warm start the inner loop.")
+    parser.add_argument("--warm_start", type=bool, default=True, help="Whether to warm start the inner loop.")
     parser.add_argument("--global_steps", type=int, default=50000,
                         help="The total number of steps to run the experiment.")
     parser.add_argument("--eval_episodes", type=int, default=100, help="The number of episodes to use for evaluation.")
@@ -46,7 +48,7 @@ def parse_args():
     parser.add_argument("--max_episode_steps", type=int, default=50, help="The maximum number of steps per episode.")
 
     # Oracle arguments.
-    parser.add_argument("--lrs", nargs='+', type=float, default=(0.001, 0.0025),
+    parser.add_argument("--lrs", nargs='+', type=float, default=(0.0005, 0.0005),
                         help="The learning rates for the models.")
     parser.add_argument("--hidden_layers", nargs='+', type=tuple, default=((64, 64), (64, 64),),
                         help="The hidden layers for the model.")
@@ -63,14 +65,14 @@ def parse_args():
     parser.add_argument("--pe_size", type=int, default=5, help="The size of the policy ensemble.")
 
     # MO-A2C specific arguments.
-    parser.add_argument("--e_coef", type=float, default=0.01, help="The entropy coefficient for A2C.")
+    parser.add_argument("--e_coef", type=float, default=0.1, help="The entropy coefficient for A2C.")
     parser.add_argument("--v_coef", type=float, default=0.5, help="The value coefficient for A2C.")
-    parser.add_argument("--max_grad_norm", type=float, default=0.5,
+    parser.add_argument("--max_grad_norm", type=float, default=50,
                         help="The maximum norm for the gradient clipping.")
     parser.add_argument("--normalize_advantage", type=bool, default=True,
                         help="Whether to normalize the advantages in A2C.")
-    parser.add_argument("--n_steps", type=int, default=10, help="The number of steps for the n-step A2C.")
-    parser.add_argument("--gae_lambda", type=float, default=0.95, help="The lambda parameter for the GAE.")
+    parser.add_argument("--n_steps", type=int, default=16, help="The number of steps for the n-step A2C.")
+    parser.add_argument("--gae_lambda", type=float, default=0.5, help="The lambda parameter for the GAE.")
 
     args = parser.parse_args()
     return args
@@ -80,6 +82,22 @@ if __name__ == '__main__':
     args = parse_args()
     run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
 
+    if args.track:
+        wandb.init(
+            project=args.wandb_project_name,
+            entity=args.wandb_entity,
+            sync_tensorboard=True,
+            config=vars(args),
+            name=run_name,
+            monitor_gym=True,
+            save_code=True,
+        )
+    writer = SummaryWriter(f"runs/{run_name}")
+    writer.add_text(
+        "hyperparameters",
+        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+    )
+
     # Seeding
     torch.manual_seed(args.seed)
     np.random.seed(args.seed)
@@ -91,6 +109,7 @@ if __name__ == '__main__':
                                        ideals=[np.array([124.0, -19.]), np.array([0., 0.])])
     oracle = init_oracle(args.oracle,
                          env,
+                         writer,
                          aug=args.aug,
                          lrs=args.lrs,
                          hidden_layers=args.hidden_layers,
@@ -112,6 +131,7 @@ if __name__ == '__main__':
                          num_objectives,
                          oracle,
                          linear_solver,
+                         writer,
                          warm_start=args.warm_start,
                          seed=args.seed)
     pf = ol.solve()
@@ -119,3 +139,5 @@ if __name__ == '__main__':
     print("Pareto front:")
     for point in pf:
         print(point)
+
+    writer.close()
diff --git a/experiments/ppo_dst.py b/experiments/ppo_dst.py
index 5a01675..059d00c 100644
--- a/experiments/ppo_dst.py
+++ b/experiments/ppo_dst.py
@@ -65,7 +65,7 @@ def parse_args():
     # MO-A2C specific arguments.
     parser.add_argument("--e_coef", type=float, default=0.01, help="The entropy coefficient for PPO.")
     parser.add_argument("--v_coef", type=float, default=0.5, help="The value coefficient for PPO.")
-    parser.add_argument("--num_envs", type=int, default=4, help="The number of environments to use.")
+    parser.add_argument("--num_envs", type=int, default=16, help="The number of environments to use.")
     parser.add_argument("--num_minibatches", type=int, default=4, help="The number of minibatches to use.")
     parser.add_argument("--update_epochs", type=int, default=4, help="The number of epochs to use for the update.")
     parser.add_argument("--max_grad_norm", type=float, default=0.5,
@@ -74,7 +74,7 @@ def parse_args():
                         help="Whether to normalize the advantages in A2C.")
     parser.add_argument("--clip_coef", type=float, default=0.2, help="The clipping coefficient for PPO.")
     parser.add_argument("--clip_vloss", type=bool, default=False, help="Whether to clip the value loss in PPO.")
-    parser.add_argument("--n_steps", type=int, default=124, help="The number of steps for the n-step PPO.")
+    parser.add_argument("--n_steps", type=int, default=128, help="The number of steps for the n-step PPO.")
     parser.add_argument("--gae_lambda", type=float, default=0.95, help="The lambda parameter for the GAE.")
     parser.add_argument("--eps", type=float, default=1e-5, help="The epsilon parameter for the Adam optimizer.")
 
diff --git a/oracles/mo_a2c.py b/oracles/mo_a2c.py
index bfa5b8e..aa00c25 100644
--- a/oracles/mo_a2c.py
+++ b/oracles/mo_a2c.py
@@ -3,7 +3,6 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
-from torch.distributions.categorical import Categorical as CDist
 from oracles.policy import Categorical
 from oracles.drl_oracle import DRLOracle
 from oracles.replay_buffer import RolloutBuffer
@@ -12,10 +11,10 @@ from oracles.replay_buffer import RolloutBuffer
 class Actor(nn.Module):
     def __init__(self, input_dim, hidden_dims, output_dim):
         super().__init__()
-        self.layers = [nn.Linear(input_dim, hidden_dims[0]), nn.ReLU()]
+        self.layers = [nn.Linear(input_dim, hidden_dims[0]), nn.Tanh()]
 
         for hidden_in, hidden_out in zip(hidden_dims[:-1], hidden_dims[1:]):
-            self.layers.extend([nn.Linear(hidden_in, hidden_out), nn.ReLU()])
+            self.layers.extend([nn.Linear(hidden_in, hidden_out), nn.Tanh()])
 
         self.layers.append(nn.Linear(hidden_dims[-1], output_dim))
         self.layers = nn.Sequential(*self.layers)
@@ -30,10 +29,10 @@ class Actor(nn.Module):
 class Critic(nn.Module):
     def __init__(self, input_dim, hidden_dims, output_dim):
         super().__init__()
-        self.layers = [nn.Linear(input_dim, hidden_dims[0]), nn.ReLU()]
+        self.layers = [nn.Linear(input_dim, hidden_dims[0]), nn.Tanh()]
 
         for hidden_in, hidden_out in zip(hidden_dims[:-1], hidden_dims[1:]):
-            self.layers.extend([nn.Linear(hidden_in, hidden_out), nn.ReLU()])
+            self.layers.extend([nn.Linear(hidden_in, hidden_out), nn.Tanh()])
 
         self.layers.append(nn.Linear(hidden_dims[-1], output_dim))
         self.layers = nn.Sequential(*self.layers)
@@ -45,6 +44,7 @@ class Critic(nn.Module):
 class MOA2C(DRLOracle):
     def __init__(self,
                  env,
+                 writer,
                  aug=0.2,
                  lrs=(0.001, 0.001),
                  hidden_layers=((64, 64), (64, 64)),
@@ -60,7 +60,7 @@ class MOA2C(DRLOracle):
                  eval_episodes=100,
                  log_freq=1000,
                  seed=0):
-        super().__init__(env, aug=aug, gamma=gamma, one_hot=one_hot, eval_episodes=eval_episodes)
+        super().__init__(env, writer, aug=aug, gamma=gamma, one_hot=one_hot, eval_episodes=eval_episodes)
 
         if len(lrs) == 1:  # Use same learning rate for all models.
             lrs = (lrs[0], lrs[0])
@@ -104,6 +104,8 @@ class MOA2C(DRLOracle):
                                             action_dtype=int,
                                             aug_obs=True)
 
+        self.estimated_values = {}
+
     def reset(self):
         """Reset the actor and critic networks, optimizers and policy."""
         self.actor = Actor(self.input_dim, self.actor_layers, self.actor_output_dim)
@@ -113,6 +115,7 @@ class MOA2C(DRLOracle):
         self.policy = Categorical()
         self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr)
         self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)
+        self.estimated_values = {}
 
     @staticmethod
     def init_weights(m, bias_const=0.01):
@@ -138,35 +141,20 @@ class MOA2C(DRLOracle):
             "seed": self.seed
         }
 
-    def calc_returns(self, rewards, dones, v_preds):
-        """Compute the returns from the rewards and values.
-
-        Args:
-            rewards (Tensor): The rewards.
-            dones (Tensor): The dones.
-            v_preds (Tensor): The predicted values for the observations and final next observation.
-
-        Returns:
-            Tensor: The returns.
-        """
-        returns = torch.zeros_like(rewards)
-        returns[-1] = rewards[-1] + self.gamma * v_preds[-1] * (1 - dones[-1])
-        for t in reversed(range(len(rewards) - 1)):
-            returns[t] = rewards[t] + self.gamma * (1 - dones[t]) * returns[t + 1]
-        return returns
-
-    def calc_generalised_advantages(self, rewards, dones, v_preds):
+    def calc_generalised_advantages(self, rewards, dones, values, v_next):
         """Compute the advantages for the rollouts.
 
         Args:
             rewards (Tensor): The rewards.
             dones (Tensor): The dones.
-            v_preds (Tensor): The predicted values for the observations and final next observation.
+            values (Tensor): The values.
+            v_next (Tensor): The value of the next state.
 
         Returns:
             Tensor: The advantages.
         """
-        td_errors = rewards + self.gamma * (1 - dones) * v_preds[1:] - v_preds[:-1]
+        v_comb = torch.cat((values, v_next), dim=0)
+        td_errors = rewards + self.gamma * (1 - dones) * v_comb[1:] - v_comb[:-1]
         advantages = torch.zeros_like(td_errors)
         advantages[-1] = td_errors[-1]
         for t in reversed(range(len(td_errors) - 1)):
@@ -181,28 +169,38 @@ class MOA2C(DRLOracle):
         v_s0.requires_grad = True
         self.u_func(v_s0).backward()  # Gradient of utility function w.r.t. values.
 
-        v_preds = self.critic(torch.cat((aug_obs, aug_next_obs[-1:]), dim=0))  # Predict values of observations.
-        returns = self.calc_returns(rewards, dones, v_preds)
-        advantages = self.calc_generalised_advantages(rewards, dones, v_preds)
+        values = self.critic(aug_obs)  # Predict values of observations.
+        with torch.no_grad():
+            v_next = self.critic(aug_next_obs[-1:])  # Predict values of next observations.
+            advantages = self.calc_generalised_advantages(rewards, dones, values, v_next)
+            returns = advantages + values
 
         if self.normalize_advantage:
             advantages = (advantages - advantages.mean(dim=0)) / (advantages.std(dim=0) + 1e-8)
 
-        dist = CDist(logits=self.actor(aug_obs))  # Distribution over actions.
-        log_prob = dist.log_prob(actions).unsqueeze(1)  # Log probability of actions.
+        actor_out = self.actor(aug_obs)  # Predict logits of actions.
+        log_prob, entropy = self.policy.evaluate_actions(actor_out, actions)  # Evaluate actions.
         pg_loss = -(advantages * log_prob).mean(dim=0)  # Policy gradient loss with advantage as baseline.
         policy_loss = torch.dot(v_s0.grad, pg_loss)  # Gradient update rule for SER.
-        entropy_loss = -torch.mean(dist.entropy())
-        value_loss = F.mse_loss(returns, v_preds[:-1])
+        entropy_loss = -torch.mean(entropy)  # Compute entropy bonus.
+        value_loss = F.mse_loss(returns, values)
 
+        loss = policy_loss + self.v_coef * value_loss + self.e_coef * entropy_loss
         self.actor_optimizer.zero_grad()
         self.critic_optimizer.zero_grad()
-        loss = policy_loss + self.v_coef * value_loss + self.e_coef * entropy_loss
         loss.backward()
-        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
-        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
-        self.actor_optimizer.step()
+        a_gnorm = self._compute_grad_norm(self.actor)
+        c_gnorm = self._compute_grad_norm(self.critic)
+        nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
+        nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
         self.critic_optimizer.step()
+        self.actor_optimizer.step()
+
+        with torch.no_grad():  # Compute utility of the policy estimated by the critic. Used for logging.
+            v_s0 = self.critic(self.s0)  # Value of s0.
+            utility = self.u_func(v_s0).item()  # Utility of the policy.
+
+        return utility, v_s0, loss.item(), policy_loss.item(), value_loss.item(), entropy_loss.item(), a_gnorm, c_gnorm
 
     def reset_env(self):
         """Reset the environment.
@@ -215,8 +213,7 @@ class MOA2C(DRLOracle):
         accrued_reward = np.zeros(self.num_objectives)
         aug_obs = torch.tensor(np.concatenate((obs, accrued_reward)), dtype=torch.float)  # Create the augmented state.
         timestep = 0
-        self.s0 = aug_obs
-        return obs, accrued_reward, aug_obs, timestep
+        return aug_obs, accrued_reward, timestep
 
     def select_action(self, aug_obs):
         """Select an action from the policy.
@@ -231,24 +228,23 @@ class MOA2C(DRLOracle):
         action = self.policy(log_probs).item()  # Sample an action from the distribution.
         return action
 
-    def select_greedy_action(self, obs, accrued_reward):
+    def select_greedy_action(self, aug_obs):
         """Select a greedy action. Used by the solve method in the super class.
 
         Args:
-            obs (Tensor): The observation.
-            accrued_reward (Tensor): The accrued reward.
+            aug_obs (Tensor): The augmented observation.
 
         Returns:
             int: The action.
         """
-        aug_obs = torch.tensor(np.concatenate((obs, accrued_reward)), dtype=torch.float)
         log_probs = self.actor(aug_obs)  # Logprobs for the actions.
         action = self.policy.greedy(log_probs).item()  # Sample an action from the distribution.
         return action
 
     def train(self):
         """Train the agent."""
-        obs, accrued_reward, aug_obs, timestep = self.reset_env()
+        aug_obs, accrued_reward, timestep = self.reset_env()
+        self.s0 = aug_obs
 
         for global_step in range(self.global_steps):
             if global_step % self.log_freq == 0:
@@ -264,14 +260,22 @@ class MOA2C(DRLOracle):
             self.rollout_buffer.add(aug_obs, action, reward, aug_next_obs, terminated or truncated)
 
             if (global_step + 1) % self.n_steps == 0:
-                self.update_policy()
+                utility, v_s0, loss, pg_l, v_l, e_l, a_gnorm, c_gnorm = self.update_policy()
+                self.writer.add_scalar(f'losses/{self.iteration}/utility', utility, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/loss', loss, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/policy_gradient_loss', pg_l, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/value_loss', v_l, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/entropy_loss', e_l, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/actor_grad_norm', a_gnorm, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/critic_grad_norm', c_gnorm, global_step)
+                self.estimated_values[global_step] = np.asarray(v_s0)
                 self.rollout_buffer.reset()
 
             aug_obs = aug_next_obs
             timestep += 1
 
             if terminated or truncated:  # If the episode is done, reset the environment and accrued reward.
-                obs, accrued_reward, aug_obs, timestep = self.reset_env()
+                aug_obs, accrued_reward, timestep = self.reset_env()
 
     def load_model(self, referent, load_actor=False, load_critic=True):
         """Load the model that is closest to the given referent.
@@ -291,11 +295,22 @@ class MOA2C(DRLOracle):
             if load_critic:
                 self.critic.load_state_dict(critic_net)
 
+    def log_distances(self, pareto_point):
+        """Log the distance of the estimated values to the retrieved pareto point.
+
+        Args:
+            pareto_point (ndarray): The pareto point.
+        """
+        distances = np.linalg.norm(np.array(list(self.estimated_values.values())) - pareto_point, axis=1)
+        for step, dist in zip(self.estimated_values.keys(), distances):
+            self.writer.add_scalar(f'losses/distance', dist, step)
+
     def solve(self, referent, ideal, warm_start=True):
         """Train the algorithm on the given environment."""
         self.reset()
         if warm_start:
             self.load_model(referent)
         pareto_point = super().solve(referent, ideal)
+        self.log_distances(pareto_point)
         self.trained_models[tuple(referent)] = (self.actor.state_dict(), self.critic.state_dict())
         return pareto_point
diff --git a/oracles/mo_ppo.py b/oracles/mo_ppo.py
index 82ce734..02d2adb 100644
--- a/oracles/mo_ppo.py
+++ b/oracles/mo_ppo.py
@@ -330,6 +330,7 @@ class MOPPO(DRLOracle):
                 timesteps = (timesteps + 1) * (1 - dones)
 
             self.update_policy()
+            self.rollout_buffer.reset()
 
     def load_model(self, referent, load_actor=False, load_critic=True):
         """Load the model that is closest to the given referent.

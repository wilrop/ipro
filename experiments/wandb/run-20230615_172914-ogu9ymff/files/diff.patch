diff --git a/experiments/a2c_dst.py b/experiments/a2c_dst.py
index 1957cab..354d7f4 100644
--- a/experiments/a2c_dst.py
+++ b/experiments/a2c_dst.py
@@ -3,6 +3,7 @@ import random
 import torch
 import argparse
 import time
+import wandb
 
 import numpy as np
 
@@ -11,6 +12,7 @@ from experiments import setup_env
 from linear_solvers import init_linear_solver
 from oracles import init_oracle
 from outer_loops import init_outer_loop
+from torch.utils.tensorboard import SummaryWriter
 
 
 def parse_args():
@@ -24,7 +26,7 @@ def parse_args():
                         help="if toggled, `torch.backends.cudnn.deterministic=False`")
     parser.add_argument("--cuda", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
                         help="if toggled, cuda will be enabled by default")
-    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=False, nargs="?", const=True,
+    parser.add_argument("--track", type=lambda x: bool(strtobool(x)), default=True, nargs="?", const=True,
                         help="if toggled, this experiment will be tracked with Weights and Biases")
     parser.add_argument("--wandb-project-name", type=str, default="PRIOL", help="the wandb's project name")
     parser.add_argument("--wandb-entity", type=str, default=None, help="the entity (team) of wandb's project")
@@ -36,7 +38,7 @@ def parse_args():
     parser.add_argument("--env_id", type=str, default="deep-sea-treasure-concave-v0", help="The game to use.")
     parser.add_argument('--outer_loop', type=str, default='2D', help='The outer loop to use.')
     parser.add_argument("--oracle", type=str, default="MO-A2C", help="The algorithm to use.")
-    parser.add_argument("--aug", type=float, default=0.001, help="The augmentation term in the utility function.")
+    parser.add_argument("--aug", type=float, default=0.01, help="The augmentation term in the utility function.")
     parser.add_argument("--tolerance", type=float, default="1e-4", help="The tolerance for the outer loop.")
     parser.add_argument("--warm_start", type=bool, default=False, help="Whether to warm start the inner loop.")
     parser.add_argument("--global_steps", type=int, default=50000,
@@ -46,7 +48,7 @@ def parse_args():
     parser.add_argument("--max_episode_steps", type=int, default=50, help="The maximum number of steps per episode.")
 
     # Oracle arguments.
-    parser.add_argument("--lrs", nargs='+', type=float, default=(0.001, 0.0025),
+    parser.add_argument("--lrs", nargs='+', type=float, default=(0.0003, 0.0003),
                         help="The learning rates for the models.")
     parser.add_argument("--hidden_layers", nargs='+', type=tuple, default=((64, 64), (64, 64),),
                         help="The hidden layers for the model.")
@@ -67,9 +69,9 @@ def parse_args():
     parser.add_argument("--v_coef", type=float, default=0.5, help="The value coefficient for A2C.")
     parser.add_argument("--max_grad_norm", type=float, default=0.5,
                         help="The maximum norm for the gradient clipping.")
-    parser.add_argument("--normalize_advantage", type=bool, default=True,
+    parser.add_argument("--normalize_advantage", type=bool, default=False,
                         help="Whether to normalize the advantages in A2C.")
-    parser.add_argument("--n_steps", type=int, default=10, help="The number of steps for the n-step A2C.")
+    parser.add_argument("--n_steps", type=int, default=16, help="The number of steps for the n-step A2C.")
     parser.add_argument("--gae_lambda", type=float, default=0.95, help="The lambda parameter for the GAE.")
 
     args = parser.parse_args()
@@ -80,6 +82,22 @@ if __name__ == '__main__':
     args = parse_args()
     run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
 
+    if args.track:
+        wandb.init(
+            project=args.wandb_project_name,
+            entity=args.wandb_entity,
+            sync_tensorboard=True,
+            config=vars(args),
+            name=run_name,
+            monitor_gym=True,
+            save_code=True,
+        )
+    writer = SummaryWriter(f"runs/{run_name}")
+    writer.add_text(
+        "hyperparameters",
+        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+    )
+
     # Seeding
     torch.manual_seed(args.seed)
     np.random.seed(args.seed)
@@ -91,6 +109,7 @@ if __name__ == '__main__':
                                        ideals=[np.array([124.0, -19.]), np.array([0., 0.])])
     oracle = init_oracle(args.oracle,
                          env,
+                         writer,
                          aug=args.aug,
                          lrs=args.lrs,
                          hidden_layers=args.hidden_layers,
@@ -112,6 +131,7 @@ if __name__ == '__main__':
                          num_objectives,
                          oracle,
                          linear_solver,
+                         writer,
                          warm_start=args.warm_start,
                          seed=args.seed)
     pf = ol.solve()
@@ -119,3 +139,5 @@ if __name__ == '__main__':
     print("Pareto front:")
     for point in pf:
         print(point)
+
+    writer.close()
diff --git a/experiments/ppo_dst.py b/experiments/ppo_dst.py
index 5a01675..059d00c 100644
--- a/experiments/ppo_dst.py
+++ b/experiments/ppo_dst.py
@@ -65,7 +65,7 @@ def parse_args():
     # MO-A2C specific arguments.
     parser.add_argument("--e_coef", type=float, default=0.01, help="The entropy coefficient for PPO.")
     parser.add_argument("--v_coef", type=float, default=0.5, help="The value coefficient for PPO.")
-    parser.add_argument("--num_envs", type=int, default=4, help="The number of environments to use.")
+    parser.add_argument("--num_envs", type=int, default=16, help="The number of environments to use.")
     parser.add_argument("--num_minibatches", type=int, default=4, help="The number of minibatches to use.")
     parser.add_argument("--update_epochs", type=int, default=4, help="The number of epochs to use for the update.")
     parser.add_argument("--max_grad_norm", type=float, default=0.5,
@@ -74,7 +74,7 @@ def parse_args():
                         help="Whether to normalize the advantages in A2C.")
     parser.add_argument("--clip_coef", type=float, default=0.2, help="The clipping coefficient for PPO.")
     parser.add_argument("--clip_vloss", type=bool, default=False, help="Whether to clip the value loss in PPO.")
-    parser.add_argument("--n_steps", type=int, default=124, help="The number of steps for the n-step PPO.")
+    parser.add_argument("--n_steps", type=int, default=128, help="The number of steps for the n-step PPO.")
     parser.add_argument("--gae_lambda", type=float, default=0.95, help="The lambda parameter for the GAE.")
     parser.add_argument("--eps", type=float, default=1e-5, help="The epsilon parameter for the Adam optimizer.")
 
diff --git a/oracles/drl_oracle.py b/oracles/drl_oracle.py
index c451f53..84e6f05 100644
--- a/oracles/drl_oracle.py
+++ b/oracles/drl_oracle.py
@@ -8,6 +8,7 @@ from gymnasium.spaces import Box
 class DRLOracle:
     def __init__(self,
                  env,
+                 writer,
                  aug=0.2,
                  gamma=0.99,
                  one_hot=False,
@@ -34,6 +35,9 @@ class DRLOracle:
         self.u_func = None
         self.trained_models = {}  # Collection of trained models that can be used for warm-starting.
 
+        self.iteration = 0
+        self.writer = writer
+
     def reset(self):
         """Reset the environment and the agent."""
         raise NotImplementedError
@@ -42,6 +46,15 @@ class DRLOracle:
         """Select the greedy action for the given observation."""
         raise NotImplementedError
 
+    @staticmethod
+    def _compute_grad_norm(model):
+        """Compute the gradient norm of the model parameters."""
+        total_norm = 0
+        for p in model.parameters():
+            param_norm = p.grad.data.norm(2)
+            total_norm += param_norm.item() ** 2
+        return total_norm ** (1. / 2)
+
     def one_hot_encode(self, obs):
         """One-hot encode the given observation.
 
@@ -122,9 +135,13 @@ class DRLOracle:
 
     def solve(self, referent, ideal):
         """Run the inner loop of the outer loop."""
+        self.writer.add_text('referent', str(referent), self.iteration)
+        self.writer.add_text('ideal', str(ideal), self.iteration)
         referent = torch.tensor(referent)
         ideal = torch.tensor(ideal)
         self.u_func = create_batched_aasf(referent, referent, ideal, aug=self.aug, backend='torch')
         self.train()
         pareto_point = self.evaluate()
+        self.writer.add_text('pareto_point', str(pareto_point), self.iteration)
+        self.iteration += 1
         return pareto_point
diff --git a/oracles/mo_a2c.py b/oracles/mo_a2c.py
index bfa5b8e..5e7b495 100644
--- a/oracles/mo_a2c.py
+++ b/oracles/mo_a2c.py
@@ -45,6 +45,7 @@ class Critic(nn.Module):
 class MOA2C(DRLOracle):
     def __init__(self,
                  env,
+                 writer,
                  aug=0.2,
                  lrs=(0.001, 0.001),
                  hidden_layers=((64, 64), (64, 64)),
@@ -60,7 +61,7 @@ class MOA2C(DRLOracle):
                  eval_episodes=100,
                  log_freq=1000,
                  seed=0):
-        super().__init__(env, aug=aug, gamma=gamma, one_hot=one_hot, eval_episodes=eval_episodes)
+        super().__init__(env, writer, aug=aug, gamma=gamma, one_hot=one_hot, eval_episodes=eval_episodes)
 
         if len(lrs) == 1:  # Use same learning rate for all models.
             lrs = (lrs[0], lrs[0])
@@ -199,11 +200,18 @@ class MOA2C(DRLOracle):
         self.critic_optimizer.zero_grad()
         loss = policy_loss + self.v_coef * value_loss + self.e_coef * entropy_loss
         loss.backward()
-        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
-        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
+        a_gnorm = self._compute_grad_norm(self.actor)
+        c_gnorm = self._compute_grad_norm(self.critic)
+        nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
+        nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
         self.actor_optimizer.step()
         self.critic_optimizer.step()
 
+        with torch.no_grad():
+            v_s0 = self.critic(self.s0)  # Value of s0.
+            utility = self.u_func(v_s0).item()  # Utility of the policy.
+        return utility, loss.item(), policy_loss.item(), value_loss.item(), entropy_loss.item(), a_gnorm, c_gnorm
+
     def reset_env(self):
         """Reset the environment.
 
@@ -264,7 +272,14 @@ class MOA2C(DRLOracle):
             self.rollout_buffer.add(aug_obs, action, reward, aug_next_obs, terminated or truncated)
 
             if (global_step + 1) % self.n_steps == 0:
-                self.update_policy()
+                utility, loss, pg_l, v_l, e_l, a_gnorm, c_gnorm = self.update_policy()
+                self.writer.add_scalar(f'losses/{self.iteration}/utility', utility, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/loss', loss, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/policy_gradient_loss', pg_l, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/value_loss', v_l, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/entropy_loss', e_l, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/actor_grad_norm', a_gnorm, global_step)
+                self.writer.add_scalar(f'losses/{self.iteration}/critic_grad_norm', c_gnorm, global_step)
                 self.rollout_buffer.reset()
 
             aug_obs = aug_next_obs
@@ -291,11 +306,47 @@ class MOA2C(DRLOracle):
             if load_critic:
                 self.critic.load_state_dict(critic_net)
 
+    def evaluate2(self):
+        """Evaluate MODQN on the given environment."""
+        pareto_point = np.zeros(self.num_objectives)
+
+        for episode in range(1000):
+            raw_obs, _ = self.env.reset()
+            obs = self.format_obs(raw_obs)
+            terminated = False
+            truncated = False
+            accrued_reward = np.zeros(self.num_objectives)
+            timestep = 0
+
+            while not (terminated or truncated):
+                aug_obs = torch.tensor(np.concatenate((obs, accrued_reward)), dtype=torch.float)
+                action = self.select_action(aug_obs)
+                next_raw_obs, reward, terminated, truncated, _ = self.env.step(action)
+                next_obs = self.format_obs(next_raw_obs)
+                accrued_reward += (self.gamma ** timestep) * reward
+                obs = next_obs
+                timestep += 1
+
+            pareto_point += accrued_reward
+
+        return pareto_point / 1000
+
     def solve(self, referent, ideal, warm_start=True):
         """Train the algorithm on the given environment."""
         self.reset()
         if warm_start:
             self.load_model(referent)
         pareto_point = super().solve(referent, ideal)
+        """with torch.no_grad():
+            v_s0 = self.critic(self.s0)  # Value of s0.
+            utility = self.u_func(v_s0).item()  # Utility of the policy.
+            actual_u = self.u_func(torch.tensor(pareto_point)).item()
+            weird_point = self.evaluate2()
+            print(f'Estimated point: {v_s0}')
+            print(f'Estimated Utility: {utility}')
+            print(f'Pareto point: {pareto_point}')
+            print(f'Pareto utility: {actual_u}')
+            print(f'Stochastic point: {weird_point}')
+            print(f'Stochastic utility: {self.u_func(torch.tensor(weird_point)).item()}')"""
         self.trained_models[tuple(referent)] = (self.actor.state_dict(), self.critic.state_dict())
         return pareto_point
diff --git a/oracles/mo_ppo.py b/oracles/mo_ppo.py
index 82ce734..02d2adb 100644
--- a/oracles/mo_ppo.py
+++ b/oracles/mo_ppo.py
@@ -330,6 +330,7 @@ class MOPPO(DRLOracle):
                 timesteps = (timesteps + 1) * (1 - dones)
 
             self.update_policy()
+            self.rollout_buffer.reset()
 
     def load_model(self, referent, load_actor=False, load_critic=True):
         """Load the model that is closest to the given referent.
diff --git a/outer_loops/__init__.py b/outer_loops/__init__.py
index 5992130..e72842e 100644
--- a/outer_loops/__init__.py
+++ b/outer_loops/__init__.py
@@ -2,7 +2,7 @@ from outer_loops.priol import Priol
 from outer_loops.priol_2D import Priol2D
 
 
-def init_outer_loop(alg, problem, objectives, oracle, linear_solver, **kwargs):
+def init_outer_loop(alg, problem, objectives, oracle, linear_solver, writer, **kwargs):
     """Initialise an outer loop.
 
     Args:
@@ -18,8 +18,8 @@ def init_outer_loop(alg, problem, objectives, oracle, linear_solver, **kwargs):
     if alg == '2D':
         if objectives != 2:
             raise ValueError('The 2D outer loop can only be used for 2D problems.')
-        return Priol2D(problem, oracle, linear_solver, **kwargs)
+        return Priol2D(problem, oracle, linear_solver, writer, **kwargs)
     elif alg == 'PRIOL':
-        return Priol(problem, objectives, oracle, linear_solver, **kwargs)
+        return Priol(problem, objectives, oracle, linear_solver, writer, **kwargs)
     else:
         raise ValueError(f'Unknown outer loop algorithm: {alg}')
diff --git a/outer_loops/priol.py b/outer_loops/priol.py
index 1278498..fb1b4b9 100644
--- a/outer_loops/priol.py
+++ b/outer_loops/priol.py
@@ -14,6 +14,7 @@ class Priol:
                  dimensions,
                  oracle,
                  linear_solver,
+                 writer,
                  warm_start=False,
                  tolerance=1e-1,
                  max_steps=5000,
@@ -58,6 +59,8 @@ class Priol:
         self.seed = seed
         self.rng = rng if rng is not None else np.random.default_rng(seed)
 
+        self.writer = writer
+
     def reset(self):
         """Reset the algorithm to its initial state."""
         self.bounding_box = None
@@ -251,11 +254,11 @@ class Priol:
         else:
             raise ValueError(f'Unknown method {method}')
 
-    def solve(self, log_freq=1, update_freq=50):
+    def solve(self, update_freq=50):
         """Solve the problem.
 
         Args:
-            log_freq (int, optional): The frequency of logging the progress of the algorithm. Defaults to 10.
+            update_freq (int, optional): The frequency of updates. Defaults to 50.
 
         Returns:
             set: The Pareto front.
@@ -270,14 +273,12 @@ class Priol:
             return {tuple(vec) for vec in self.pf}
 
         while self.error_estimates[-1] > self.tol and step < self.max_steps:
-            if step % log_freq == 0:
-                print(f'Step {step}')
-                print(f'↪ Covered volume: {self.covered_volume * 100:.5f}%')
-                print(f'↪ Error estimate: {self.error_estimates[-1]:.5f}')
+            begin_loop = time.time()
+            print(f'Step {step} - Covered {self.covered_volume * 100:.5f}% - Error {self.error_estimates[-1]:.5f}')
+
 
             referent = self.select_referent(method='first')
             vec = self.oracle.solve(np.copy(referent), np.copy(self.ideal), warm_start=self.warm_start)
-            print(f'Referent {referent} -> Vec {vec}')
 
             if strict_pareto_dominates(vec, referent):
                 self.pf = np.vstack((self.pf, vec))
@@ -297,6 +298,8 @@ class Priol:
 
             self.estimate_error()
             step += 1
+            print(f'Ref {referent} - Found {vec} - Time {time.time() - begin_loop:.2f}s')
+            print('---------------------')
 
         pf = {tuple(vec) for vec in extreme_prune(np.vstack((self.pf, self.robust_points)))}
 
diff --git a/outer_loops/priol_2D.py b/outer_loops/priol_2D.py
index 9668711..234a523 100644
--- a/outer_loops/priol_2D.py
+++ b/outer_loops/priol_2D.py
@@ -10,7 +10,7 @@ from utils.pareto import p_prune, strict_pareto_dominates
 class Priol2D:
     """An inner-outer loop method for solving 2D multi-objective problems."""
 
-    def __init__(self, problem, oracle, linear_solver, warm_start=False, tolerance=1e-6, seed=None):
+    def __init__(self, problem, oracle, linear_solver, writer, warm_start=False, tolerance=1e-6, seed=None):
         self.problem = problem
         self.oracle = oracle
         self.linear_solver = linear_solver
@@ -25,6 +25,8 @@ class Priol2D:
         self.error_estimates = []
         self.covered_volume = 0
 
+        self.writer = writer
+
     def reset(self):
         """Reset the algorithm to its initial state."""
         self.bounding_box = None
@@ -130,21 +132,24 @@ class Priol2D:
         """Check if the algorithm is done."""
         return not self.box_queue or self.error_estimates[-1] <= self.tolerance
 
-    def solve(self, log_freq=1):
-        """Solve the problem."""
+    def solve(self):
+        """Solve the problem.
+
+        Returns:
+            set: The Pareto front.
+        """
         start = time.time()
         self.init_phase()
         step = 0
 
         while not self.is_done():
-            if step % log_freq == 0:
-                print(f'Step {step} - Covered volume: {self.percentage_covered():.5f}%')
+            begin_loop = time.time()
+            print(f'Step {step} - Covered {self.percentage_covered():.5f}% - Error {self.error_estimates[-1]:.5f}')
 
             box = self.get_next_box()
             ideal = np.copy(box.ideal)
             referent = np.copy(box.nadir)
             vec = self.oracle.solve(referent, ideal, warm_start=self.warm_start)
-            print(f'Referent {referent} -> Vec {vec}')
 
             if strict_pareto_dominates(vec, referent):  # Check that new point is valid.
                 self.update(box, vec)
@@ -153,6 +158,8 @@ class Priol2D:
                 self.pf.add(tuple(vec))  # Add vec to the PF for robustness. It'll get pruned anyway if it's dominated.
             self.estimate_error()
             step += 1
+            print(f'Ref {referent} - Found {vec} - Time {time.time() - begin_loop:.2f}s')
+            print('---------------------')
 
         pf = p_prune(self.pf.copy())
 
